{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码和说明来源：\n",
    "# https://www.jiqizhixin.com/articles/2019-02-19-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # 公式 (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # 公式 (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # 公式 (2) 所需，边上的用户定义函数\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e' : F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # 公式 (3), (4)所需，传递消息用的用户定义函数\n",
    "        return {'z' : edges.src['z'], 'e' : edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # 公式 (3), (4)所需, 归约用的用户定义函数\n",
    "        # 公式 (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # 公式 (4)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # 公式 (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # 公式 (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # 公式 (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力 (Multi-head attention)\n",
    "\n",
    "神似卷积神经网络里的多通道，GAT 引入了多头注意力来丰富模型的能力和稳定训练的过程。每一个注意力的头都有它自己的参数。如何整合多个注意力机制的输出结果一般有两种方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # 对输出特征维度（第1维）做拼接\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # 用求平均整合多头结果\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 Cora 数据集上训练一个 GAT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个两层的 GAT 模型：\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        # 注意输入的维度是 hidden_dim * num_heads 因为多头的结果都被拼接在了\n",
    "        # 一起。 此外输出层只有一个头。\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用 DGL 自带的数据模块加载 Cora 数据集。\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()                     # num_nodes=2708, num_edges=10556\n",
    "    features = torch.FloatTensor(data.features)    #2708 , 1433\n",
    "    labels = torch.LongTensor(data.labels)         #2708  [int]==》目标变量的取值\n",
    "    mask = torch.ByteTensor(data.train_mask)       #2708  [1,0]===>标签？\n",
    "    g = DGLGraph(data.graph)\n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, features, labels, mask = load_cora_data()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练的流程和 GCN 教程里的一样。（样例数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voyager/anaconda3/envs/graph_pytorch/lib/python3.6/site-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n",
      "/home/voyager/anaconda3/envs/graph_pytorch/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/voyager/anaconda3/envs/graph_pytorch/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.9462 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.9445 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.9429 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.9412 | Time(s) 0.2891\n",
      "Epoch 00004 | Loss 1.9395 | Time(s) 0.2896\n",
      "Epoch 00005 | Loss 1.9377 | Time(s) 0.2900\n",
      "Epoch 00006 | Loss 1.9359 | Time(s) 0.2925\n",
      "Epoch 00007 | Loss 1.9340 | Time(s) 0.2944\n",
      "Epoch 00008 | Loss 1.9321 | Time(s) 0.2937\n",
      "Epoch 00009 | Loss 1.9302 | Time(s) 0.2929\n",
      "Epoch 00010 | Loss 1.9282 | Time(s) 0.2939\n",
      "Epoch 00011 | Loss 1.9261 | Time(s) 0.2947\n",
      "Epoch 00012 | Loss 1.9240 | Time(s) 0.2969\n",
      "Epoch 00013 | Loss 1.9218 | Time(s) 0.2967\n",
      "Epoch 00014 | Loss 1.9196 | Time(s) 0.2961\n",
      "Epoch 00015 | Loss 1.9173 | Time(s) 0.2957\n",
      "Epoch 00016 | Loss 1.9149 | Time(s) 0.2956\n",
      "Epoch 00017 | Loss 1.9125 | Time(s) 0.2957\n",
      "Epoch 00018 | Loss 1.9100 | Time(s) 0.2955\n",
      "Epoch 00019 | Loss 1.9074 | Time(s) 0.2951\n",
      "Epoch 00020 | Loss 1.9048 | Time(s) 0.2946\n",
      "Epoch 00021 | Loss 1.9020 | Time(s) 0.2946\n",
      "Epoch 00022 | Loss 1.8993 | Time(s) 0.2951\n",
      "Epoch 00023 | Loss 1.8964 | Time(s) 0.2948\n",
      "Epoch 00024 | Loss 1.8935 | Time(s) 0.2951\n",
      "Epoch 00025 | Loss 1.8904 | Time(s) 0.2950\n",
      "Epoch 00026 | Loss 1.8873 | Time(s) 0.2949\n",
      "Epoch 00027 | Loss 1.8842 | Time(s) 0.2947\n",
      "Epoch 00028 | Loss 1.8809 | Time(s) 0.2946\n",
      "Epoch 00029 | Loss 1.8776 | Time(s) 0.2945\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "g, features, labels, mask = load_cora_data()\n",
    "\n",
    "# 创建模型\n",
    "net = GAT(g, \n",
    "          in_dim=features.size()[1], \n",
    "          hidden_dim=8, \n",
    "          out_dim=7, \n",
    "          num_heads=8)\n",
    "#print(net)\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# 主流程\n",
    "dur = []\n",
    "for epoch in range(30):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    logits = net(features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[mask], labels[mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 兰州数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
